{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic Regression with Tfidf.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "tm3ezeQqNkLE",
        "colab_type": "code",
        "outputId": "81787ec9-b1be-4283-d623-feca1c57a427",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "# 挂载 google 云盘\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# 将文件路径指定为数据路径\n",
        "import os\n",
        "os.chdir('/content/gdrive/My Drive/comment')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm #显示运行进度条\n",
        "\n",
        "# # 画图库\n",
        "# %matplotlib inline\n",
        "# %config InlineBackend.figure_format = 'retina'\n",
        "# import seaborn as sb\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# 文字处理库\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer \n",
        "# nltk.download('stopwords') #下载停止词\n",
        "import string\n",
        "from keras.preprocessing import sequence\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "7OkHYg13N9g7",
        "colab_type": "code",
        "outputId": "25c82aaf-925b-4918-e793-f8c49e31f18d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords') #下载停止词\n",
        "train_data = pd.read_csv('train.csv')\n",
        "test_data = pd.read_csv('test.csv')\n",
        "labels = train_data.columns[2:].tolist()\n",
        "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
        "all_comments = pd.concat([train_data['comment_text'], test_data['comment_text']])\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.update(string.punctuation)\n",
        "comments_refined = []\n",
        "for line in tqdm(all_comments):\n",
        "    comments_refined.append(' '.join([word for word in tokenizer.tokenize(line) if word not in stop_words]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 312735/312735 [00:10<00:00, 30463.12it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "nbA6dL6tOUUC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "outputId": "b0e3c525-811f-493c-82cf-9c2ef94d46b3"
      },
      "cell_type": "code",
      "source": [
        "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "vectorizer = TfidfVectorizer(\n",
        "    strip_accents='unicode',\n",
        "    analyzer='word',\n",
        "    use_idf=True,\n",
        "    smooth_idf=True,\n",
        "    sublinear_tf=True,\n",
        "    max_features=50000).fit(comments_refined)\n",
        "train_features, test_features = map(lambda x: vectorizer.transform(x), [train_data['comment_text'], test_data['comment_text']] )\n",
        "scores = []\n",
        "submission = pd.DataFrame.from_dict({'id': test_data['id']})\n",
        "for label in tqdm(labels):\n",
        "    classifier = LogisticRegression(C=4, \n",
        "                                    solver='sag')\n",
        "    cv_score = np.mean(cross_val_score(classifier, \n",
        "                                       train_features, \n",
        "                                       train_data[label], \n",
        "                                       cv=3, \n",
        "                                       scoring='roc_auc'))\n",
        "    scores.append(cv_score)\n",
        "    print('CV score for class {} is {}'.format(label, cv_score))\n",
        "\n",
        "    classifier.fit(train_features, train_data[label])\n",
        "    submission[label] = classifier.predict_proba(test_features)[:, 1]\n",
        "\n",
        "print('Total CV score is {}'.format(np.mean(scores)))\n",
        "submission.to_csv('submission_simple.csv', index=False)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CV score for class toxic is 0.973382006386684\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            " 17%|█▋        | 1/6 [00:20<01:41, 20.25s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CV score for class severe_toxic is 0.9832314692514882\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            " 33%|███▎      | 2/6 [00:44<01:25, 21.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CV score for class obscene is 0.9850783263974195\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            " 50%|█████     | 3/6 [01:03<01:02, 20.81s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CV score for class threat is 0.9858184648000871\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            " 67%|██████▋   | 4/6 [01:23<00:40, 20.47s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CV score for class insult is 0.9775646210344795\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            " 83%|████████▎ | 5/6 [01:45<00:20, 20.94s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CV score for class identity_hate is 0.9737676642141246\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "100%|██████████| 6/6 [02:06<00:00, 21.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total CV score is 0.9798070920140471\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lta7uTZWj6-a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#   0%|          | 0/6 [00:00<?, ?it/s]CV score for class toxic is 0.973382006386684\n",
        "#  17%|█▋        | 1/6 [00:20<01:41, 20.25s/it]CV score for class severe_toxic is 0.9832314692514882\n",
        "#  33%|███▎      | 2/6 [00:44<01:25, 21.34s/it]CV score for class obscene is 0.9850783263974195\n",
        "#  50%|█████     | 3/6 [01:03<01:02, 20.81s/it]CV score for class threat is 0.9858184648000871\n",
        "#  67%|██████▋   | 4/6 [01:23<00:40, 20.47s/it]CV score for class insult is 0.9775646210344795\n",
        "#  83%|████████▎ | 5/6 [01:45<00:20, 20.94s/it]CV score for class identity_hate is 0.9737676642141246\n",
        "# 100%|██████████| 6/6 [02:06<00:00, 21.05s/it]\n",
        "# Total CV score is 0.9798070920140471\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}