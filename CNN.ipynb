{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "RnXq6kMAe6Id",
        "colab_type": "code",
        "outputId": "940e4ec6-2779-4cad-ae6a-df6cb220bacc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D\n",
        "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.callbacks import Callback, EarlyStopping\n",
        "\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import os\n",
        "os.chdir('/content/gdrive/My Drive/comment')\n",
        "from keras.layers import Embedding, Conv1D, Dropout, Dense, SpatialDropout1D, BatchNormalization, Activation\n",
        "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D, MaxPooling1D, AveragePooling1D\n",
        "from keras import optimizers, metrics, Model\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import regularizers\n",
        "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
        "from keras.layers import Concatenate, Input, Flatten\n",
        "from keras.utils import plot_model\n",
        "from sklearn.metrics import roc_curve,auc,roc_auc_score\n",
        "from keras.callbacks import Callback\n",
        "from keras import initializers\n",
        "from keras.layers import CuDNNGRU\n",
        "from tqdm import tqdm\n",
        "import io\n",
        "\n",
        "# 画图库\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 文字处理库\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer \n",
        "nltk.download('stopwords') #下载停止词\n",
        "import string\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import text_to_word_sequence"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "5fvkGh_bqGCF",
        "colab_type": "code",
        "outputId": "9198426b-3fdb-4d08-acb3-2346f42b33cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print('正在加载词向量......')\n",
        "fin = io.open('crawl-300d-2M.vec', 'r', encoding='utf-8')\n",
        "embeddings = {} #词向量字典\n",
        "word_features = 0 #词向量维度\n",
        "for line in tqdm(fin):\n",
        "    tokens =  line.rstrip().rsplit(' ')\n",
        "    embeddings[tokens[0]] = np.asarray(tokens[1:], dtype='float32')\n",
        "    word_features = max(word_features, len(tokens[1:]))\n",
        "fin.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "正在加载词向量......\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1999996it [03:45, 8852.02it/s] \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "4SVhXeicqNng",
        "colab_type": "code",
        "outputId": "8ba06e02-e1bc-4736-d244-04a25b2063ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('train.csv')\n",
        "test_data = pd.read_csv('test.csv')\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "# # stop_words.update(string.punctuation)\n",
        "train_refined, test_refined = [], []\n",
        "# # 元素化评论,过滤评论中停止词及标点\n",
        "# # [tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')]\n",
        "\n",
        "print('训练集文本处理中......')\n",
        "for line in tqdm(train_data['comment_text']):\n",
        "    word_seq = text_to_word_sequence(line,\n",
        "                              filters=string.punctuation+'\\n\\t0123456789',\n",
        "                              lower=True,\n",
        "                              split=\" \")\n",
        "    train_refined.append(' '.join([word for word in word_seq]))\n",
        "print('测试集文本处理中......')\n",
        "for line in tqdm(test_data['comment_text']):\n",
        "    word_seq = text_to_word_sequence(line,\n",
        "                              filters=string.punctuation+'\\n\\t0123456789',\n",
        "                              lower=True,\n",
        "                              split=\" \")\n",
        "    test_refined.append(' '.join([word for word in word_seq]))\n",
        "# train_refined = list(train_data['comment_text'])\n",
        "# test_refined = list(test_data['comment_text'])\n",
        "\n",
        "\n",
        "word_upper = 100000 # 词量上限\n",
        "# 基于词频，保留前 100000 个词\n",
        "tokenizer = Tokenizer(num_words=word_upper)\n",
        "# 创建评论词典，一个单词对应一个索引\n",
        "tokenizer.fit_on_texts(train_refined + test_refined)\n",
        "token_train, token_test = map(lambda x: tokenizer.texts_to_sequences(tqdm(x)), [train_refined, test_refined])\n",
        "word_index = tokenizer.word_index\n",
        "print('评论中的词汇量为 {}'.format(len(word_index)))\n",
        "maxlen = 200\n",
        "x_train, x_test = map(lambda x: sequence.pad_sequences(tqdm(x), maxlen=maxlen), [token_train, token_test])\n",
        "feature_matrix = np.zeros((min(word_upper, len(word_index)), word_features))\n",
        "for word, index in tqdm(word_index.items()):\n",
        "    if index >= min(word_upper, len(word_index)):\n",
        "      continue\n",
        "    word_vec = embeddings.get(word)\n",
        "    if word_vec is not None:\n",
        "        feature_matrix[index] = word_vec\n",
        "        \n",
        "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "y_train = train_data[labels].values\n",
        "embedding_matrix = feature_matrix"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  1%|▏         | 2275/159571 [00:00<00:06, 22746.12it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "训练集文本处理中......\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 159571/159571 [00:06<00:00, 25517.79it/s]\n",
            "  2%|▏         | 2363/153164 [00:00<00:06, 23627.26it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "测试集文本处理中......\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 153164/153164 [00:05<00:00, 26493.83it/s]\n",
            "100%|██████████| 159571/159571 [00:10<00:00, 14593.33it/s]\n",
            "100%|██████████| 153164/153164 [00:09<00:00, 15360.74it/s]\n",
            "100%|██████████| 159571/159571 [00:00<00:00, 1315279.70it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "评论中的词汇量为 341884\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 153164/153164 [00:00<00:00, 1375800.70it/s]\n",
            "100%|██████████| 341884/341884 [00:00<00:00, 609963.67it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "_DlsRH96gkzj",
        "colab_type": "code",
        "outputId": "3549f750-b9f8-446d-ed79-9cc200b835fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# EMBEDDING_FILE = 'crawl-300d-2M.vec'\n",
        "\n",
        "# train = pd.read_csv('train.csv')\n",
        "# test = pd.read_csv('test.csv')\n",
        "# submission = pd.read_csv('sample_submission.csv')\n",
        "# X_train = train[\"comment_text\"].fillna(\"fillna\").values\n",
        "# y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
        "# X_test = test[\"comment_text\"].fillna(\"fillna\").values\n",
        "\n",
        "# max_features = 100000\n",
        "# maxlen = 200\n",
        "# embed_size = 300\n",
        "\n",
        "# tokenizer = text.Tokenizer(num_words=max_features)\n",
        "# tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
        "# X_train = tokenizer.texts_to_sequences(X_train)\n",
        "# X_test = tokenizer.texts_to_sequences(X_test)\n",
        "# x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
        "# x_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
        "\n",
        "\n",
        "\n",
        "# def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
        "# embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in tqdm(open(EMBEDDING_FILE)))\n",
        "\n",
        "# word_index = tokenizer.word_index\n",
        "# nb_words = min(max_features, len(word_index))\n",
        "# embedding_matrix = np.zeros((nb_words, embed_size))\n",
        "# for word, i in tqdm(word_index.items()):\n",
        "#     if i >= max_features: continue\n",
        "#     embedding_vector = embeddings_index.get(word)\n",
        "#     if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1999996it [03:16, 10160.76it/s]\n",
            "100%|██████████| 394787/394787 [00:00<00:00, 841687.84it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "5UlcccvGXaFn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# pickle.dump({'xtrain': x_train, 'xtest': x_test, 'em':embedding_matrix, 'ytrain':y_train}, open('/content/gdrive/My Drive/comment/varibles.txt', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iwafHuvmVLRy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def add_features(df):\n",
        "    \n",
        "    df['comment_text'] = df['comment_text'].apply(lambda x:str(x))\n",
        "    df['total_length'] = df['comment_text'].apply(len)\n",
        "    df['capitals'] = df['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
        "    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/float(row['total_length']),\n",
        "                                axis=1)\n",
        "    df['num_words'] = df.comment_text.str.count('\\S+')\n",
        "    df['num_unique_words'] = df['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
        "    df['words_vs_unique'] = df['num_unique_words'] / df['num_words'] \n",
        "    df['mark'] = df.comment_text.str.count('!')\n",
        "    df['mark_rate'] = df.apply(lambda row: float(row['mark'])/float(row['total_length']),\n",
        "                                axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "train = add_features(train)\n",
        "test = add_features(test)\n",
        "\n",
        "features = train[['caps_vs_length', 'mark_rate']].fillna(0)\n",
        "test_features = test[['caps_vs_length', 'mark_rate']].fillna(0)\n",
        "\n",
        "ss = StandardScaler()\n",
        "ss.fit(np.vstack((features, test_features)))\n",
        "features = ss.transform(features)\n",
        "test_features = ss.transform(test_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HVZibDQBfb9t",
        "colab_type": "code",
        "outputId": "0729b9ec-9775-47e6-b248-09a4e92a7647",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        }
      },
      "cell_type": "code",
      "source": [
        "# train_data = pd.read_csv('train.csv')\n",
        "# test_data = pd.read_csv('test.csv')\n",
        "# pickle.dump({'strain': seq_train, 'stest': seq_test, 'fm':feature_matrix}, open('/content/gdrive/My Drive/comment/var.txt', 'wb'))\n",
        "var = pickle.load(open('/content/gdrive/My Drive/comment/varibles.txt', 'rb'))\n",
        "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "x_train = var['xtrain']\n",
        "x_test = var['xtest']\n",
        "embedding_matrix = var['em']\n",
        "y_train = var['ytrain']\n",
        "\n",
        "class RocAucEvaluation(Callback):\n",
        "    def __init__(self, validation_data=(), interval=1):\n",
        "        super(Callback, self).__init__()\n",
        "\n",
        "        self.interval = interval\n",
        "        self.X_val, self.y_val = validation_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if epoch % self.interval == 0:\n",
        "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
        "            score = roc_auc_score(self.y_val, y_pred)\n",
        "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
        "\n",
        "\n",
        "\n",
        "max_features = 100000\n",
        "embed_size = 300\n",
        "maxlen = 200\n",
        "\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
        "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
        "\n",
        "num_filters = 64\n",
        "filter_sizes = [1,2,3]\n",
        "\n",
        "\n",
        "#--------------------------------------\n",
        "# features_input = Input(shape=(features.shape[1],))\n",
        "\n",
        "#--------------------------------------\n",
        "\n",
        "\n",
        "inp = Input(shape=(maxlen, ))\n",
        "features_input = Input(shape=(features.shape[1],))\n",
        "\n",
        "x = Embedding(max_features, \n",
        "              embed_size,  \n",
        "              input_length = maxlen,\n",
        "              weights=[embedding_matrix],\n",
        "              trainable=True)(inp)\n",
        "\n",
        "x = SpatialDropout1D(0.5)(x)\n",
        "\n",
        "x = Bidirectional(CuDNNGRU(128, \n",
        "                           return_sequences=True))(x)   #adding gru layer\n",
        "\n",
        "conv_0 = Conv1D(num_filters, \n",
        "                filter_sizes[0], \n",
        "                kernel_initializer=initializers.RandomNormal())(x)\n",
        "conv_1 = Conv1D(num_filters, \n",
        "                filter_sizes[1], \n",
        "                kernel_initializer=initializers.RandomNormal())(x)\n",
        "conv_2 = Conv1D(num_filters, \n",
        "                filter_sizes[2], \n",
        "                kernel_initializer=initializers.RandomNormal())(x)\n",
        "\n",
        "\n",
        "ac_0 = PReLU()(conv_0)\n",
        "ac_1 = PReLU()(conv_1)\n",
        "ac_2 = PReLU()(conv_2)\n",
        "\n",
        "# bn_0 = BatchNormalization()(ac_0)\n",
        "# bn_1 = BatchNormalization()(ac_1)\n",
        "# bn_2 = BatchNormalization()(ac_2)\n",
        "\n",
        "\n",
        "maxpool_0 = MaxPooling1D(pool_size=(maxlen - filter_sizes[0] + 1))(ac_0)\n",
        "maxpool_1 = MaxPooling1D(pool_size=(maxlen - filter_sizes[1] + 1))(ac_1)\n",
        "maxpool_2 = MaxPooling1D(pool_size=(maxlen - filter_sizes[2] + 1))(ac_2)\n",
        "\n",
        "\n",
        "z = Concatenate(axis=1)([maxpool_0,\n",
        "                         maxpool_1,\n",
        "                         maxpool_2\n",
        "                        ])\n",
        "\n",
        "z = BatchNormalization()(z)\n",
        "z = Flatten()(z)\n",
        "conc = Concatenate(axis=1)([z,\n",
        "#                             x_h,\n",
        "                            features_input])\n",
        "conc = Dropout(0.2)(conc)   #0.2 0.9834\n",
        "\n",
        "\n",
        "outp = Dense(6, kernel_initializer=initializers.RandomNormal(), activation='sigmoid')(conc)\n",
        "\n",
        "model = Model(\n",
        "              [inp,features_input],\n",
        "              outp)\n",
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# inp = Input(shape=(maxlen, ))\n",
        "\n",
        "# x = Embedding(max_features, \n",
        "#               embed_size,  \n",
        "#               input_length = maxlen,\n",
        "#               weights=[embedding_matrix],\n",
        "#               trainable=True)(inp)\n",
        "\n",
        "# x = SpatialDropout1D(0.5)(x)\n",
        "# x = Bidirectional(GRU(128, return_sequences=True))(x)\n",
        "\n",
        "# avg_pool = GlobalAveragePooling1D()(x)\n",
        "# max_pool = GlobalMaxPooling1D()(x)\n",
        "\n",
        "# conc = Concatenate(axis=1)([avg_pool, max_pool])\n",
        "# outp = Dense(6, activation=\"sigmoid\")(conc)\n",
        "\n",
        "# model = Model(inputs=inp, outputs=outp)\n",
        "# model.compile(loss='binary_crossentropy',\n",
        "#               optimizer='adam',\n",
        "#               metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "batch_size = 512\n",
        "epochs = 10\n",
        "\n",
        "# X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, \n",
        "#                                               test_size=0.1, \n",
        "#                                               shuffle=True)\n",
        "mix = np.hstack((x_train, features))\n",
        "X_tra, X_val, y_tra, y_val = train_test_split(mix,\n",
        "                                              y_train, \n",
        "                                              test_size=0.1, \n",
        "                                              shuffle=True)\n",
        "# RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
        "RocAuc = RocAucEvaluation(validation_data=([X_val[:,:200],X_val[:,200:]], y_val), interval=1)\n",
        "\n",
        "\n",
        "early = EarlyStopping(monitor='val_acc', \n",
        "                      min_delta=0.01, \n",
        "                      patience=3, #4    0.9835\n",
        "                      verbose=1)\n",
        "\n",
        "callback_list = [RocAuc, early]\n",
        "\n",
        "hist = model.fit(\n",
        "#                  X_tra, \n",
        "                 [X_tra[:,:200],X_tra[:,200:]], \n",
        "                 y_tra, \n",
        "                 batch_size=batch_size, \n",
        "                 epochs=epochs, \n",
        "                 validation_data=(\n",
        "#                      X_val, y_val\n",
        "                     [X_val[:,:200],X_val[:,200:]], y_val\n",
        "                 ),\n",
        "                 callbacks=callback_list, \n",
        "                 verbose=1,\n",
        "                 shuffle=True,\n",
        "                 class_weight='balanced')\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(hist.history['loss'], lw=2.0, color='b', label='train')\n",
        "plt.plot(hist.history['val_loss'], lw=2.0, color='r', label='val')\n",
        "plt.title('CNN sentiment')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Cross-Entropy Loss')\n",
        "plt.legend(loc='upper right') \n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(hist.history['acc'], lw=2.0, color='b', label='train')\n",
        "plt.plot(hist.history['val_acc'], lw=2.0, color='r', label='val')\n",
        "plt.title('CNN sentiment')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "y_pred = model.predict(\n",
        "#                         x_test, \n",
        "                        [x_test,test_features], \n",
        "                        batch_size=1024)\n",
        "submission = pd.read_csv('sample_submission.csv')\n",
        "submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
        "# submission.to_csv('CNN.csv', index=False)\n",
        "test_labels = pd.read_csv('test_labels.csv')\n",
        "real_test = test_labels[test_labels['toxic']>=0]\n",
        "score = roc_auc_score(real_test[labels], submission.iloc[real_test.index][labels])\n",
        "print('final score is {:.4f}'.format(score))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d9bea7c64b33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/comment/varibles.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'toxic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'severe_toxic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'obscene'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'threat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'insult'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'identity_hate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'xtrain'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'xtest'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'em'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "IwhsHY0GUCS9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "submission.to_csv('CNN.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vQiEQp2o-J7P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Architecture\n",
        "First layer: concatenated fasttext and glove twitter embeddings. Fasttext vector is used by itself if there is no glove vector but not the other way around. Words without word vectors are replaced with a word vector for a word \"something\". Also, I added additional value that was set to 1 if a word was written in all capital letters and 0 otherwise.\n",
        "\n",
        "Second layer: SpatialDropout1D(0.5)\n",
        "\n",
        "Third layer: Bidirectional CuDNNLSTM with a kernel size 40. I found out that LSTM as a first layer works better than GRU.\n",
        "\n",
        "Fourth layer: Bidirectional CuDNNGRU with a kernel size 40.\n",
        "\n",
        "Fifth layer: A concatenation of the last state, maximum pool, average pool and two features: \"Unique words rate\" and \"Rate of all-caps words\"\n",
        "\n",
        "Sixth layer: output dense layer.\n",
        "\n",
        "Hyperparameters and preprocessing:\n",
        "Batch size: 512. I found that bigger batch size makes results more stable.\n",
        "Epochs: 15.\n",
        "Sequence length: 900.\n",
        "Optimizer: Adam with clipped gradient.\n",
        "Preprocessing: Unidecode library (https://pypi.python.org/pypi/Unidecode) to convert text to ASCII first and after that filtering everything except letters and some punctuation.\n",
        "Text normalization\n",
        "I did a lot of work on fixing misspellings and I think it improved the score. I was only fixing misspellings that didn't have a fasttext vector. Things that I did:\n",
        "\n",
        "Created a list of words that appear more often in toxic comments than in regular comments and words that appear more often in non-toxic comments. For every misspelled word I looked up if it has a word in the list with a small Levenshtein distance to it.\n",
        "Fixed some misspellings with TextBlob dictionary.\n",
        "Fixed misspellings by finding word vector neighborhoods. Fasttext tool can create vectors for out-of-dictionary words which is really nice. I trained my own fasttext vectors on Wikipedia comments corpus and used them to do this. I also used those vectors as embeddings but results were not as good as with regular fasttext vectors."
      ]
    },
    {
      "metadata": {
        "id": "bRtfUOC9oEa2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}